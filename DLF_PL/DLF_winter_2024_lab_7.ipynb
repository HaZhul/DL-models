{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqPmKpT3zil0"
   },
   "source": [
    "### Audio data in Pytorch Lightning - example using simple built-in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijK8pu3Cz9Ek"
   },
   "outputs": [],
   "source": [
    "!pip install torchaudio\n",
    "!pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUz94bZ0rzPZ"
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "from lightning.pytorch.callbacks import EarlyStopping, TQDMProgressBar\n",
    "import torchmetrics\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHuL7Jqr3XOx"
   },
   "outputs": [],
   "source": [
    "# https://pytorch.org/audio/stable/generated/torchaudio.transforms.MelSpectrogram.html\n",
    "# https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html#sphx-glr-tutorials-audio-feature-extractions-tutorial-py\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Mel_scale\n",
    "\n",
    "class SubsetSC(SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None):\n",
    "        super().__init__(\"./data\", download=True)\n",
    "\n",
    "        def load_list(filename):\n",
    "            filepath = os.path.join(self._path, filename)\n",
    "            with open(filepath) as fileobj:\n",
    "                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n",
    "\n",
    "        if subset == \"validation\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\":\n",
    "            excludes = set(load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\"))\n",
    "            self._walker = [w for w in self._walker if w not in excludes]\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        fileid = self._walker[n]\n",
    "        waveform, sample_rate = torchaudio.load(fileid)\n",
    "\n",
    "        # Extract label from the file path\n",
    "        label = os.path.basename(os.path.dirname(fileid))\n",
    "\n",
    "        return waveform, sample_rate, label\n",
    "\n",
    "class AudioDataModule(L.LightningDataModule):\n",
    "    def __init__(self, batch_size=32, data_dir=\"./data\", sample_rate=16000):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.data_dir = data_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.transform = MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=1024,\n",
    "            hop_length=512,\n",
    "            n_mels=64\n",
    "        )\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pathlib.Path(self.data_dir).mkdir(parents=True, exist_ok=True)\n",
    "        SPEECHCOMMANDS(root=self.data_dir, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_data = SubsetSC(\"training\")\n",
    "        self.val_data = SubsetSC(\"validation\")\n",
    "\n",
    "        # Get the list of commands (classes)\n",
    "        self.labels = sorted([\n",
    "            'backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow',\n",
    "            'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine',\n",
    "            'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three',\n",
    "            'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero'\n",
    "        ])\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(self.labels)}\n",
    "\n",
    "    def process_audio(self, waveform, sample_rate):\n",
    "        if sample_rate != self.sample_rate:\n",
    "            resampler = Resample(sample_rate, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        mel_spec = self.transform(waveform)\n",
    "        return mel_spec.squeeze(0)\n",
    "\n",
    "    def collate_batch(self, batch):\n",
    "        waveforms = []\n",
    "        labels = []\n",
    "\n",
    "        # First, process all waveforms and find the maximum length\n",
    "        processed_waves = []\n",
    "        max_length = 0\n",
    "\n",
    "        for waveform, sample_rate, command in batch:\n",
    "            try:\n",
    "                processed = self.process_audio(waveform, sample_rate)\n",
    "                processed_waves.append(processed)\n",
    "                max_length = max(max_length, processed.size(1))\n",
    "                labels.append(self.label_to_idx[command])\n",
    "            except KeyError as e:\n",
    "                print(f\"Warning: Unknown command {command}\")\n",
    "                continue\n",
    "\n",
    "        if not processed_waves:\n",
    "            raise RuntimeError(\"No valid samples in batch\")\n",
    "\n",
    "        # Pad all spectrograms to the same length\n",
    "        padded_waves = []\n",
    "        for wave in processed_waves:\n",
    "            if wave.size(1) < max_length:\n",
    "                padding = torch.zeros(64, max_length - wave.size(1))\n",
    "                padded_wave = torch.cat([wave, padding], dim=1)\n",
    "                padded_waves.append(padded_wave)\n",
    "            else:\n",
    "                padded_waves.append(wave)\n",
    "\n",
    "        waveforms = torch.stack(padded_waves)\n",
    "        labels = torch.tensor(labels)\n",
    "        return waveforms, labels\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.collate_batch,\n",
    "            num_workers=2\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self.collate_batch,\n",
    "            num_workers=2\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3184d7R58_Of"
   },
   "outputs": [],
   "source": [
    "class AudioClassifier(L.LightningModule):\n",
    "    def __init__(self, n_classes, n_mels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=n_mels,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(256, n_classes)\n",
    "\n",
    "        metrics = torchmetrics.MetricCollection([\n",
    "            torchmetrics.Accuracy(task='multiclass', num_classes=n_classes),\n",
    "            torchmetrics.F1Score(task='multiclass', num_classes=n_classes)\n",
    "            # https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall\n",
    "            # https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.f1_score.html\n",
    "            # https://deepgram.com/ai-glossary/f1-score-machine-learning\n",
    "            # more than 0.9 -> very good; less than 0.5 - very bad\n",
    "\n",
    "        ])\n",
    "\n",
    "        self.train_metrics = metrics.clone(prefix='train_')\n",
    "        self.val_metrics = metrics.clone(prefix='val_')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        output, hidden = self.gru(x)\n",
    "        return self.fc(hidden[-1])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "\n",
    "        metrics = self.train_metrics(logits, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "\n",
    "        metrics = self.val_metrics(logits, y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AFY64qN8_R-"
   },
   "outputs": [],
   "source": [
    "class LitProgressBar(TQDMProgressBar):\n",
    "    def get_metrics(self, trainer, model):\n",
    "        items = super().get_metrics(trainer, model)\n",
    "        items.pop(\"v_num\", None)\n",
    "        return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6XkIyt08_U_"
   },
   "outputs": [],
   "source": [
    "L.seed_everything(42)\n",
    "\n",
    "data_module = AudioDataModule()\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "print(f\"Number of classes: {len(data_module.labels)}\")\n",
    "print(\"Available commands:\", data_module.labels)\n",
    "\n",
    "model = AudioClassifier(\n",
    "    n_classes=len(data_module.labels)\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        mode='min'\n",
    "    ),\n",
    "    LitProgressBar(refresh_rate=10)\n",
    "]\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator='auto',\n",
    "    callbacks=callbacks,\n",
    "    val_check_interval=0.25,\n",
    "    log_every_n_steps=10,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbuitA8k9K1p"
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPPWp7NaNZR7"
   },
   "source": [
    "### Task a:\n",
    "Change GRU layer to something you know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaTsw5z-9QQS"
   },
   "source": [
    "### Task b:\n",
    "Using the docs do the same thing with the torchtext library\n",
    "Do it using the AmazonReviewPolarity dataset\n",
    "https://pytorch.org/text/main/datasets.html#amazonreviewpolarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGKq_1faVEMB"
   },
   "outputs": [],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5z1JVtfVUZB"
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import AmazonReviewFull\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weeAyH9WdhvV"
   },
   "source": [
    "### Simple hyperparameter tuning with Ray Tune\n",
    "\n",
    "This is not the only way to tune the model, as there are multiple similiar tools (Optuna, Skorch etc).\n",
    "\n",
    "Based on: https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIM1G0h910Ib"
   },
   "outputs": [],
   "source": [
    "!pip install ray[tune]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWt987gad8uH"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ray import tune\n",
    "from ray import train\n",
    "from ray.train import Checkpoint, get_checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "# https://docs.ray.io/en/latest/tune/api/doc/ray.tune.schedulers.AsyncHyperBandScheduler.html\n",
    "import ray.cloudpickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2Optac3d8wd"
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"./data\"):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=False, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRpeZQZjd8zW"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, l1=120, l2=84):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
    "        self.fc2 = nn.Linear(l1, l2)\n",
    "        self.fc3 = nn.Linear(l2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOocrxotd80r"
   },
   "outputs": [],
   "source": [
    "def train_cifar(config, data_dir=None):\n",
    "    net = Net(config[\"l1\"], config[\"l2\"])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    checkpoint = get_checkpoint()\n",
    "    if checkpoint:\n",
    "        with checkpoint.as_directory() as checkpoint_dir:\n",
    "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "            with open(data_path, \"rb\") as fp:\n",
    "                checkpoint_state = pickle.load(fp)\n",
    "            start_epoch = checkpoint_state[\"epoch\"]\n",
    "            net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "            optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    trainset, testset = load_data(data_dir)\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs]\n",
    "    )\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
    "    )\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
    "    )\n",
    "\n",
    "    for epoch in range(start_epoch, 10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\n",
    "                    \"[%d, %5d] loss: %.3f\"\n",
    "                    % (epoch + 1, i + 1, running_loss / epoch_steps)\n",
    "                )\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.inference_mode():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"net_state_dict\": net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "            with open(data_path, \"wb\") as fp:\n",
    "                pickle.dump(checkpoint_data, fp)\n",
    "\n",
    "            checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
    "            train.report(\n",
    "                {\"loss\": val_loss / val_steps, \"accuracy\": correct / total},\n",
    "                checkpoint=checkpoint,\n",
    "            )\n",
    "\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-qplLi9d83s"
   },
   "outputs": [],
   "source": [
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.inference_mode():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LC8NX_5Td845"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"l1\": tune.choice([2 ** i for i in range(9)]),\n",
    "    \"l2\": tune.choice([2 ** i for i in range(9)]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pu1VyxAaP2u-"
   },
   "outputs": [],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    load_data(data_dir)\n",
    "    config = {\n",
    "        \"l1\": tune.choice([2**i for i in range(9)]),\n",
    "        \"l2\": tune.choice([2**i for i in range(9)]),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16]),\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "    result = tune.run(\n",
    "        partial(train_cifar, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(f\"Best trial config: {best_trial.config}\")\n",
    "    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "    print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "\n",
    "    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint = result.get_best_checkpoint(trial=best_trial, metric=\"accuracy\", mode=\"max\")\n",
    "    with best_checkpoint.as_directory() as checkpoint_dir:\n",
    "        data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "        with open(data_path, \"rb\") as fp:\n",
    "            best_checkpoint_data = pickle.load(fp)\n",
    "\n",
    "        best_trained_model.load_state_dict(best_checkpoint_data[\"net_state_dict\"])\n",
    "        test_acc = test_accuracy(best_trained_model, device)\n",
    "        print(\"Best trial test set accuracy: {}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iV4vJatgP2yQ"
   },
   "outputs": [],
   "source": [
    "main(num_samples=10, max_num_epochs=10, gpus_per_trial=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYLH-uDkWoHk"
   },
   "source": [
    "### Tuning using PyTorch Lightning\n",
    "https://docs.ray.io/en/latest/tune/examples/tune-pytorch-lightning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MjkP1O-yYBZc"
   },
   "outputs": [],
   "source": [
    "!pip install lightning\n",
    "!pip install ray[tune]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXwQf7R4WjgV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tempfile\n",
    "import lightning.pytorch as pl\n",
    "import torch.nn.functional as F\n",
    "from filelock import FileLock\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4koZkHTWWjiK"
   },
   "outputs": [],
   "source": [
    "class MNISTClassifier(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=10, top_k=1)\n",
    "        self.layer_1_size = config[\"layer_1_size\"]\n",
    "        self.layer_2_size = config[\"layer_2_size\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "\n",
    "        # mnist images are (1, 28, 28) (channels, width, height)\n",
    "        self.layer_1 = torch.nn.Linear(28 * 28, self.layer_1_size)\n",
    "        self.layer_2 = torch.nn.Linear(self.layer_1_size, self.layer_2_size)\n",
    "        self.layer_3 = torch.nn.Linear(self.layer_2_size, 10)\n",
    "        self.eval_loss = []\n",
    "        self.eval_accuracy = []\n",
    "\n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return F.nll_loss(logits, labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, width, height = x.size()\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        x = self.layer_1(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.layer_2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.layer_3(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "\n",
    "        self.log(\"ptl/train_loss\", loss)\n",
    "        self.log(\"ptl/train_accuracy\", accuracy)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "        self.eval_loss.append(loss)\n",
    "        self.eval_accuracy.append(accuracy)\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.eval_loss).mean()\n",
    "        avg_acc = torch.stack(self.eval_accuracy).mean()\n",
    "        self.log(\"ptl/val_loss\", avg_loss, sync_dist=True)\n",
    "        self.log(\"ptl/val_accuracy\", avg_acc, sync_dist=True)\n",
    "        self.eval_loss.clear()\n",
    "        self.eval_accuracy.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Homl5SFbdD1"
   },
   "outputs": [],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=128):\n",
    "        super().__init__()\n",
    "        self.data_dir = tempfile.mkdtemp()\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        with FileLock(f\"{self.data_dir}.lock\"):\n",
    "            mnist = MNIST(\n",
    "                self.data_dir, train=True, download=True, transform=self.transform\n",
    "            )\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist, [55000, 5000])\n",
    "\n",
    "            self.mnist_test = MNIST(\n",
    "                self.data_dir, train=False, download=True, transform=self.transform\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-dXITPeWjlA"
   },
   "outputs": [],
   "source": [
    "default_config = {\n",
    "    \"layer_1_size\": 128,\n",
    "    \"layer_2_size\": 256,\n",
    "    \"lr\": 1e-3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSy1ODQ8WjmP"
   },
   "outputs": [],
   "source": [
    "from ray.train.lightning import (\n",
    "    RayDDPStrategy,\n",
    "    RayLightningEnvironment,\n",
    "    RayTrainReportCallback,\n",
    "    prepare_trainer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wm96UaxUWjpY"
   },
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    dm = MNISTDataModule(batch_size=config[\"batch_size\"])\n",
    "    model = MNISTClassifier(config)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        devices=\"auto\",\n",
    "        accelerator=\"auto\",\n",
    "        strategy=RayDDPStrategy(),\n",
    "        callbacks=[RayTrainReportCallback()],\n",
    "        plugins=[RayLightningEnvironment()],\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "    trainer = prepare_trainer(trainer)\n",
    "    trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ar1LV_4_W1Wu"
   },
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler, AsyncHyperBandScheduler\n",
    "# https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/\n",
    "# https://arxiv.org/abs/1810.05934\n",
    "# https://docs.ray.io/en/latest/tune/api/doc/ray.tune.schedulers.AsyncHyperBandScheduler.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PG6d18CJW1ZF"
   },
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"layer_1_size\": tune.choice([32, 64, 128]),\n",
    "    \"layer_2_size\": tune.choice([64, 128, 256]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"batch_size\": tune.choice([32, 64]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPuPdHyRW1b1"
   },
   "outputs": [],
   "source": [
    "# The maximum training epochs\n",
    "num_epochs = 3 # 5\n",
    "\n",
    "# Number of sampls from parameter space\n",
    "num_samples = 5 # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQzP40cYW1dr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2XjO6eJcCf7"
   },
   "outputs": [],
   "source": [
    "from ray.train import RunConfig, ScalingConfig, CheckpointConfig\n",
    "\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=1, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n",
    "    # num_workers=2, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1} # based on hardware configuration; if incorrect will not run (waiting for resources)\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        num_to_keep=2,\n",
    "        checkpoint_score_attribute=\"ptl/val_accuracy\",\n",
    "        checkpoint_score_order=\"max\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AibeQBsZcCiW"
   },
   "outputs": [],
   "source": [
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "# Define a TorchTrainer without hyper-parameters for Tuner\n",
    "ray_trainer = TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrLAM02YW1g3"
   },
   "outputs": [],
   "source": [
    "# https://docs.ray.io/en/latest/tune/api/doc/ray.tune.schedulers.AsyncHyperBandScheduler.html\n",
    "\n",
    "def tune_mnist_asha(num_samples=10):\n",
    "    scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        ray_trainer,\n",
    "        param_space={\"train_loop_config\": search_space},\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"ptl/val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            num_samples=num_samples,\n",
    "            scheduler=scheduler,\n",
    "        ),\n",
    "    )\n",
    "    return tuner.fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1R-uwAKZc5Zs"
   },
   "outputs": [],
   "source": [
    "# https://docs.ray.io/en/latest/tune/api/suggestion.html#random-search-and-grid-search-tune-search-basic-variant-basicvariantgenerator\n",
    "\n",
    "results = tune_mnist_asha(num_samples=num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVa-7Ys7pNFZ"
   },
   "source": [
    "### Optimizing using CometML\n",
    "https://www.comet.com/docs/v2/guides/optimizer/quickstart/\n",
    "https://www.comet.com/docs/v2/guides/quickstart/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rj1gqEOdpT6p"
   },
   "outputs": [],
   "source": [
    "!pip install comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxwJFZw8rRXZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBB2jGuWYl0O"
   },
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "from google.colab import userdata\n",
    "\n",
    "# Initialize the Comet SDK\n",
    "\n",
    "# log in with prompt for key\n",
    "# comet_ml.login(project_name=\"DLF_lab7_hyper_autotune_quickstart\")\n",
    "\n",
    "\n",
    "# log in with file (pass path to directory with config file)\n",
    "# set up \".comet.config\" file with API key\n",
    "# create a file with following contents:\n",
    "'''\n",
    "[comet]\n",
    "api_key = <YOUR_API_KEY>\n",
    "'''\n",
    "# comet_ml.login(project_name=\"DLF_lab7_hyper_autotune_quickstart\", directory=\"./\")\n",
    "\n",
    "# log in with secret from google colab\n",
    "comet_ml.login(userdata.get('COMET_API_KEY'), project_name=\"DLF_lab7_hyper_autotune_quickstart\")\n",
    "\n",
    "\n",
    "# experiment = comet_ml.start(project_name=\"DLF_lab7_hyper_autotune_quickstart\")\n",
    "#you can rename the experiment so it will be easier to find\n",
    "# experiment.set_name(\"very_clever_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJKQ1g_przBR"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def build_model_graph(experiment):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Dense(\n",
    "            experiment.get_parameter(\"first_layer_units\"),\n",
    "            activation=\"sigmoid\",\n",
    "            input_shape=(784,),\n",
    "        )\n",
    "    )\n",
    "    model.add(Dense(128, activation=\"sigmoid\"))\n",
    "    model.add(Dense(128, activation=\"sigmoid\"))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=RMSprop(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(experiment, model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=experiment.get_parameter(\"batch_size\"),\n",
    "        epochs=experiment.get_parameter(\"epochs\"),\n",
    "        validation_data=(x_test, y_test),\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate(experiment, model, x_test, y_test):\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(\"Score %s\", score)\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    num_classes = 10\n",
    "\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    x_train = x_train.reshape(60000, 784)\n",
    "    x_test = x_test.reshape(10000, 784)\n",
    "    x_train = x_train.astype(\"float32\")\n",
    "    x_test = x_test.astype(\"float32\")\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    print(x_train.shape[0], \"train samples\")\n",
    "    print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = to_categorical(y_train, num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWXPsTCHrzEF"
   },
   "outputs": [],
   "source": [
    "# Get the dataset:\n",
    "x_train, y_train, x_test, y_test = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oInYcGERr_eD"
   },
   "outputs": [],
   "source": [
    "# The optimization config:\n",
    "config = {\n",
    "    \"algorithm\": \"bayes\",  # \"random\" ; \"grid\"\n",
    "    \"name\": \"Optimize MNIST Network - Keras implementation (MLP)\",\n",
    "    \"spec\": {\"maxCombo\": 5, \"objective\": \"minimize\", \"metric\": \"loss\"},\n",
    "    \"parameters\": {\n",
    "        \"first_layer_units\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"mu\": 500,\n",
    "            \"sigma\": 50,\n",
    "            \"scalingType\": \"normal\",\n",
    "        },\n",
    "        \"batch_size\": {\"type\": \"discrete\", \"values\": [64, 128, 256]},\n",
    "    },\n",
    "    \"trials\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\"type\": \"float\", \"scaling_type\": \"log_uniform\", \"min\": 0.00001, \"max\": 0.001},\n",
    "        \"batch_size\": {\"type\": \"discrete\", \"values\": [32, 64, 128, 256]},\n",
    "    },\n",
    "'''\n",
    "\n",
    "opt = comet_ml.Optimizer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COUIL8AorzF4"
   },
   "outputs": [],
   "source": [
    "for experiment in opt.get_experiments():\n",
    "    # Log parameters, or others:\n",
    "    experiment.log_parameter(\"epochs\", 10)\n",
    "\n",
    "    # Build the model:\n",
    "    model = build_model_graph(experiment)\n",
    "\n",
    "    # Train it:\n",
    "    train(experiment, model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "    # How well did it do?\n",
    "    evaluate(experiment, model, x_test, y_test)\n",
    "\n",
    "    # Optionally, end the experiment:\n",
    "    experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcWu9PMLsugp"
   },
   "source": [
    "### Task 1:\n",
    "\n",
    "Based on previous code create an automatic hyperparameter tuning test, using PyTorch or PyTorch Lightning, the Imaginette dataset, chosen (modifiable) network architecture and visualize the process using CometML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "import einops\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImaginetteCNN(nn.Module):\n",
    "\n",
    "    @staticmethod\n",
    "    def depthwise_separable_conv(in_channels, out_channels, kernel_size=3, stride=1, padding=1, dropout_p=0.2):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, groups=in_channels),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def __init__(self, num_classes=10, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # Entry flow\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            ImaginetteCNN.depthwise_separable_conv(64, 128),\n",
    "            nn.ReLU(),           \n",
    "            ImaginetteCNN.depthwise_separable_conv(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            ImaginetteCNN.depthwise_separable_conv(128, 256),\n",
    "            nn.ReLU(),           \n",
    "            ImaginetteCNN.depthwise_separable_conv(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            # Middle flow\n",
    "            ImaginetteCNN.depthwise_separable_conv(256, 256),\n",
    "            nn.ReLU(),           \n",
    "            ImaginetteCNN.depthwise_separable_conv(256, 256),\n",
    "            nn.ReLU(),\n",
    "            ImaginetteCNN.depthwise_separable_conv(256, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Exit flow\n",
    "            ImaginetteCNN.depthwise_separable_conv(256, 512),\n",
    "            nn.ReLU(),\n",
    "            ImaginetteCNN.depthwise_separable_conv(512, 1024),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=2048, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.features(x)\n",
    "        x = einops.rearrange(x, \"b c w h -> b (c w h)\")\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXmLkJhRrzKX"
   },
   "outputs": [],
   "source": [
    "from config import API_KEY\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as v2\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from ray import tune\n",
    "from ray.air.integrations.comet import CometLoggerCallback\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from torch.utils.data import random_split\n",
    "from ray.air import session\n",
    "\n",
    "def get_imaginette_dataloaders(batch_size):\n",
    "\n",
    "\n",
    "    transform = v2.Compose([\n",
    "            v2.RandomRotation(10),\n",
    "            v2.RandomHorizontalFlip(p=0.5),\n",
    "            v2.Resize((224,224)),\n",
    "            v2.ToTensor(),\n",
    "            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    dataset = datasets.Imagenette(root=\"C:/Users/Dominik/Documents/GitHub/DL-models/DLF_PL/data\", split=\"train\", download=False, transform=transform, size=\"full\")\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "\n",
    "def train_imaginette(config):\n",
    "    experiment = comet_ml.Experiment(\n",
    "        api_key=API_KEY,\n",
    "        project_name=\"imaginette-tuning\",\n",
    "    )\n",
    "    print(\"Starting train\")\n",
    "    \n",
    "    experiment.log_parameters(config)\n",
    "    experiment.log_parameter(\"epochs\", config[\"epochs\"])\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    train_loader, val_loader = get_imaginette_dataloaders(config[\"batch_size\"])\n",
    "\n",
    "    model = ImaginetteCNN()\n",
    "    model.to(device)\n",
    "    \n",
    "    if config[\"optimizer\"] == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        session.report({\"mean_accuracy\": accuracy})\n",
    "    \n",
    "\n",
    "    experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 10:34:22,016\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-12-03 10:47:13</td></tr>\n",
       "<tr><td>Running for: </td><td>00:12:51.81        </td></tr>\n",
       "<tr><td>Memory:      </td><td>13.3/31.8 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=5<br>Bracket: Iter 4.000: 65.62829989440338 | Iter 2.000: 53.59028511087645 | Iter 1.000: 20.327349524815205<br>Logical resource usage: 2.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">         lr</th><th>optimizer  </th><th style=\"text-align: right;\">    acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>trial_c6fa6_00000</td><td>TERMINATED</td><td>127.0.0.1:27608</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.000117176</td><td>sgd        </td><td style=\"text-align: right;\">18.2154</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        219.272 </td></tr>\n",
       "<tr><td>trial_c6fa6_00001</td><td>TERMINATED</td><td>127.0.0.1:2236 </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.00032169 </td><td>adam       </td><td style=\"text-align: right;\">70.2218</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        205.135 </td></tr>\n",
       "<tr><td>trial_c6fa6_00002</td><td>TERMINATED</td><td>127.0.0.1:14348</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.000579781</td><td>sgd        </td><td style=\"text-align: right;\">20.3273</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         44.0754</td></tr>\n",
       "<tr><td>trial_c6fa6_00003</td><td>TERMINATED</td><td>127.0.0.1:16100</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.000117934</td><td>adam       </td><td style=\"text-align: right;\">72.3865</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        211.988 </td></tr>\n",
       "<tr><td>trial_c6fa6_00004</td><td>TERMINATED</td><td>127.0.0.1:27092</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.00415227 </td><td>adam       </td><td style=\"text-align: right;\">15.9451</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         48.0485</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/hazhul/imaginette-tuning/727c2e3802e24d79bcf025fd258036fe\n",
      "\n",
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m Experiment.add_tags: parameter 'tags' must be of type(s) 'list' but None was given\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Failed to add tag(s) None to the experiment\n",
      "\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m The given value of the metric checkpoint_dir_name was None; ignoring\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th style=\"text-align: right;\">  mean_accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>trial_c6fa6_00000</td><td style=\"text-align: right;\">        18.2154</td></tr>\n",
       "<tr><td>trial_c6fa6_00001</td><td style=\"text-align: right;\">        70.2218</td></tr>\n",
       "<tr><td>trial_c6fa6_00002</td><td style=\"text-align: right;\">        20.3273</td></tr>\n",
       "<tr><td>trial_c6fa6_00003</td><td style=\"text-align: right;\">        72.3865</td></tr>\n",
       "<tr><td>trial_c6fa6_00004</td><td style=\"text-align: right;\">        15.9451</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m The given value of the metric checkpoint_dir_name was None; ignoring\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m The given value of the metric checkpoint_dir_name was None; ignoring\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m The given value of the metric checkpoint_dir_name was None; ignoring\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m The given value of the metric checkpoint_dir_name was None; ignoring\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : trial_c6fa6_00000\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/hazhul/imaginette-tuning/727c2e3802e24d79bcf025fd258036fe\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     iterations_since_restore [5] : (1, 5)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_accuracy [5]            : (11.562829989440338, 18.215417106652588)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time_since_restore [5]       : (48.427513122558594, 219.27188563346863)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time_this_iter_s [5]         : (42.260950803756714, 48.427513122558594)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time_total_s [5]             : (48.427513122558594, 219.27188563346863)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     timestamp [5]                : (1733218516, 1733218687)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     training_iteration [5]       : (1, 5)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Created from : Ray\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name         : trial_c6fa6_00000\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trial_id     : c6fa6_00000\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs     : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr         : 0.00011717619832447486\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer  : sgd\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   System Information:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     date     : 2024-12-03_10-38-07\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hostname : DESKTOP-J42HTEU\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     node_ip  : 127.0.0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pid      : 27608\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 6 metrics, params and output messages\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/hazhul/imaginette-tuning/d3010b68c512425e9de00b8336c429d5\n",
      "\n",
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m Experiment.add_tags: parameter 'tags' must be of type(s) 'list' but None was given\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Failed to add tag(s) None to the experiment\n",
      "\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m The given value of the metric checkpoint_dir_name was None; ignoring\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m The given value of the metric checkpoint_dir_name was None; ignoring\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m The given value of the metric checkpoint_dir_name was None; ignoring\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m The given value of the metric checkpoint_dir_name was None; ignoring\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m The given value of the metric checkpoint_dir_name was None; ignoring\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : trial_c6fa6_00001\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/hazhul/imaginette-tuning/d3010b68c512425e9de00b8336c429d5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     iterations_since_restore [5] : (1, 5)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_accuracy [5]            : (38.859556494192184, 70.22175290390707)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time_since_restore [5]       : (45.98297309875488, 205.13457584381104)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time_this_iter_s [5]         : (38.747090339660645, 45.98297309875488)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time_total_s [5]             : (45.98297309875488, 205.13457584381104)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     timestamp [5]                : (1733218742, 1733218901)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     training_iteration [5]       : (1, 5)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Created from : Ray\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name         : trial_c6fa6_00001\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trial_id     : c6fa6_00001\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs     : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr         : 0.00032168985629382356\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer  : adam\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   System Information:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     date     : 2024-12-03_10-41-41\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hostname : DESKTOP-J42HTEU\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     node_ip  : 127.0.0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pid      : 2236\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/hazhul/imaginette-tuning/b6b161029be3477bbbb5c4754cf53e37\n",
      "\n",
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m Experiment.add_tags: parameter 'tags' must be of type(s) 'list' but None was given\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Failed to add tag(s) None to the experiment\n",
      "\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m The given value of the metric checkpoint_dir_name was None; ignoring\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : trial_c6fa6_00002\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/hazhul/imaginette-tuning/b6b161029be3477bbbb5c4754cf53e37\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     iterations_since_restore : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_accuracy            : 20.327349524815205\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time_since_restore       : 44.07542943954468\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time_this_iter_s         : 44.07542943954468\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time_total_s             : 44.07542943954468\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     timestamp                : 1733218953\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     training_iteration       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Created from : Ray\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name         : trial_c6fa6_00002\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trial_id     : c6fa6_00002\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs     : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr         : 0.0005797806148405015\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer  : sgd\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   System Information:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     date     : 2024-12-03_10-42-33\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hostname : DESKTOP-J42HTEU\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     node_ip  : 127.0.0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pid      : 14348\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 14 metrics, params and output messages\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/hazhul/imaginette-tuning/a71654be89694898a9802ce74d1f4e6c\n",
      "\n",
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m Experiment.add_tags: parameter 'tags' must be of type(s) 'list' but None was given\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Failed to add tag(s) None to the experiment\n",
      "\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m The given value of the metric checkpoint_dir_name was None; ignoring\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m The given value of the metric checkpoint_dir_name was None; ignoring\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m The given value of the metric checkpoint_dir_name was None; ignoring\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m The given value of the metric checkpoint_dir_name was None; ignoring\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m The given value of the metric checkpoint_dir_name was None; ignoring\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : trial_c6fa6_00003\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/hazhul/imaginette-tuning/a71654be89694898a9802ce74d1f4e6c\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     iterations_since_restore [5] : (1, 5)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_accuracy [5]            : (48.25765575501584, 72.38648363252376)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time_since_restore [5]       : (46.73088884353638, 211.98824214935303)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time_this_iter_s [5]         : (41.06899404525757, 46.73088884353638)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time_total_s [5]             : (46.73088884353638, 211.98824214935303)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     timestamp [5]                : (1733219008, 1733219174)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     training_iteration [5]       : (1, 5)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Created from : Ray\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name         : trial_c6fa6_00003\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trial_id     : c6fa6_00003\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs     : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr         : 0.00011793360955543408\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer  : adam\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   System Information:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     date     : 2024-12-03_10-46-14\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hostname : DESKTOP-J42HTEU\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     node_ip  : 127.0.0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pid      : 16100\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 6 metrics, params and output messages\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for assets to finish uploading (timeout is 10800 seconds)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m All assets have been sent, waiting for delivery confirmation\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/hazhul/imaginette-tuning/b669266da4f14ac196b08da9f9eddb7a\n",
      "\n",
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m Experiment.add_tags: parameter 'tags' must be of type(s) 'list' but None was given\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Failed to add tag(s) None to the experiment\n",
      "\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m The given value of the metric checkpoint_dir_name was None; ignoring\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : trial_c6fa6_00004\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/hazhul/imaginette-tuning/b669266da4f14ac196b08da9f9eddb7a\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     iterations_since_restore : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_accuracy            : 15.945089757127771\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time_since_restore       : 48.04848885536194\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time_this_iter_s         : 48.04848885536194\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time_total_s             : 48.04848885536194\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     timestamp                : 1733219231\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     training_iteration       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Created from : Ray\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name         : trial_c6fa6_00004\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trial_id     : c6fa6_00004\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs     : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr         : 0.004152271989577199\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer  : adam\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   System Information:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     date     : 2024-12-03_10-47-11\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hostname : DESKTOP-J42HTEU\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     node_ip  : 127.0.0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pid      : 27092\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 4 metrics, params and output messages\n",
      "2024-12-03 10:47:13,836\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/Dominik/ray_results/train_imaginette_2024-12-03_10-34-22' in 0.0080s.\n",
      "2024-12-03 10:47:13,841\tINFO tune.py:1041 -- Total run time: 771.83 seconds (771.80 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'lr': 0.00011793360955543408, 'batch_size': 16, 'optimizer': 'adam', 'epochs': 5}\n",
      "Best trial final validation accuracy: 72.38648363252376\n"
     ]
    }
   ],
   "source": [
    "def custom_trial_name_creator(trial):\n",
    "    return f\"trial_{trial.trial_id}\"\n",
    "\n",
    "def custom_trial_dirname_creator(trial):\n",
    "    return f\"trial_{trial.trial_id}\"\n",
    "\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "    \"batch_size\": tune.choice([16, 32, 64]),\n",
    "    \"optimizer\": tune.choice([\"adam\", \"sgd\"]),\n",
    "    \"epochs\": 5,\n",
    "}\n",
    "    \n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"mean_accuracy\",\n",
    "    mode=\"max\",\n",
    "    max_t=5,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2\n",
    ")\n",
    "    \n",
    "result = tune.run(\n",
    "    tune.with_parameters(train_imaginette),\n",
    "    resources_per_trial={\"cpu\": 2, \"gpu\": 1 if torch.cuda.is_available() else 0},\n",
    "    config=config,\n",
    "    num_samples=5,\n",
    "    scheduler=scheduler,\n",
    "    trial_name_creator=custom_trial_name_creator,\n",
    "    trial_dirname_creator=custom_trial_dirname_creator,\n",
    "    callbacks=[CometLoggerCallback(api_key=API_KEY,\n",
    "                                project_name=\"imaginette-tuning\")]\n",
    "    )\n",
    "    \n",
    "best_trial = result.get_best_trial(\"mean_accuracy\", \"max\", \"last\") # or 'all' - ask!!\n",
    "print(f\"Best trial config: {best_trial.config}\")\n",
    "print(f\"Best trial final validation accuracy: {best_trial.last_result['mean_accuracy']}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
